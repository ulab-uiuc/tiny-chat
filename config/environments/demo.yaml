models:
  model1:
    name: "gpt-4o-mini"
    type: "openai"
    temperature: 0.7
    max_tokens: 1000
    timeout: 30
  
  model2:
    name: "gpt-4o"
    type: "openai"
    temperature: 0.1
    max_tokens: 1000
    timeout: 30

  # Anthropic models (uncomment to use)
  # claude-3-sonnet:
  #   name: claude-3-sonnet-20240229
  #   type: anthropic
  #   temperature: 0.7
  #   timeout: 45
  
  # Together AI models (uncomment to use)
  # llama-2-70b-together:
  #   name: meta-llama/Llama-2-70b-chat-hf
  #   type: together
  #   temperature: 0.7
  #   timeout: 45
  
  # vLLM models (uncomment to use)
  # llama-2-7b-vllm:
  #   name: llama-2-7b-chat
  #   type: vllm
  #   api_base: http://localhost:8000
  #   temperature: 0.8
  #   timeout: 30

evaluators:
  - type: "rule_based"
    config:
      max_turn_number: 20
      max_stale_turn: 2
    enabled: true
  
  - type: "llm"
    model: "model2"
    config:
      dimensions: "sotopia"
    enabled: true

default_model: "model1"
action_order: "agent_id_based"
speaking_order: [1, 2, 0]
available_action_types:
  - "none"
  - "speak"
  - "non-verbal communication"
  - "action"
  - "leave"
max_turns: 4
# sync_mode: true

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

api:
  host: "0.0.0.0"
  port: 8000
  workers: 1
  reload: false
  cors_origins: ["*"]
  rate_limit: "100/minute"